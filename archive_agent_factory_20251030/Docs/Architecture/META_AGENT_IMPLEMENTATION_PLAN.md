# ðŸš€ Meta-Agent Implementation Plan

**Target Environment**: MacBook Pro M4, 16GB RAM, LM Studio with MLX-optimized models

---

## ðŸ“‹ Table of Contents

1. [System Requirements & Optimization](#system-requirements--optimization)
2. [LLM Model Selection](#llm-model-selection)
3. [Architecture Overview](#architecture-overview)
4. [Implementation Phases](#implementation-phases)
5. [Detailed Setup Instructions](#detailed-setup-instructions)
6. [Resource Management](#resource-management)
7. [Testing Strategy](#testing-strategy)
8. [Timeline & Milestones](#timeline--milestones)

---

## 1. System Requirements & Optimization

### **1.1 Hardware Profile**

```
MacBook Pro M4 Specifications:
â”œâ”€ CPU: Apple M4 (10-core)
â”œâ”€ GPU: Apple M4 GPU (10-core)
â”œâ”€ RAM: 16GB Unified Memory
â”œâ”€ Neural Engine: 16-core
â””â”€ Storage: SSD (assuming 512GB+)

Memory Allocation Strategy (16GB total):
â”œâ”€ macOS System: ~3GB
â”œâ”€ LM Studio + Model: ~6-8GB
â”œâ”€ PostgreSQL: ~1GB
â”œâ”€ Meta-Agent + Python: ~2GB
â”œâ”€ Docker (for sandbox): ~2GB
â””â”€ Available: ~2GB buffer

Critical: Close unnecessary applications during operation
```

### **1.2 Software Requirements**

```
Operating System:
â”œâ”€ macOS Sonoma 14.0+ (recommended for M4)
â””â”€ Ensure Xcode Command Line Tools installed

Core Components:
â”œâ”€ Python 3.11 or 3.12 (avoid 3.13, compatibility issues)
â”œâ”€ LM Studio (latest version with MLX support)
â”œâ”€ PostgreSQL (already installed)
â”œâ”€ Docker Desktop for Mac (Apple Silicon version)
â”œâ”€ Homebrew (package manager)
â””â”€ Git

Python Libraries:
â”œâ”€ langchain (0.1.0+)
â”œâ”€ langchain-openai (for LM Studio compatibility)
â”œâ”€ langgraph (0.0.20+)
â”œâ”€ pydantic (2.5.0+)
â”œâ”€ sqlalchemy (2.0.23+)
â”œâ”€ psycopg2-binary (2.9.9+)
â”œâ”€ loguru (0.7.2+)
â”œâ”€ pyyaml (6.0+)
â”œâ”€ pytest (7.4.0+)
â””â”€ httpx (0.25.0+)
```

### **1.3 M4 Optimization**

```
Leverage Apple Silicon:
â”œâ”€ Use MLX-optimized models (faster inference)
â”œâ”€ Enable Metal GPU acceleration
â”œâ”€ Use unified memory efficiently
â””â”€ Leverage Neural Engine where possible

macOS Settings:
â”œâ”€ Disable automatic graphics switching (use High Performance)
â”œâ”€ Increase file descriptor limits
â”œâ”€ Enable Docker resource limits
â””â”€ Configure swap space (if needed)

Terminal Commands:
  # Increase file descriptor limit
  ulimit -n 10240
  
  # Check available memory
  vm_stat
  
  # Monitor GPU usage
  sudo powermetrics --samplers gpu_power
```

---

## 2. LLM Model Selection

### **2.1 Model Comparison for Meta-Agent**

| Model | Size | RAM Usage | Tokens/sec (M4) | Code Quality | Availability |
|-------|------|-----------|-----------------|--------------|--------------|
| **qwen2.5-coder-7b-instruct-mlx** | 7B | ~6GB | ~40-50 | Excellent | âœ“ |
| **deepseek-coder-6.7b-instruct-mlx** | 6.7B | ~5.5GB | ~45-55 | Excellent | âœ“ |
| **codellama-7b-instruct-mlx** | 7B | ~6GB | ~35-45 | Good | âœ“ |
| **mistral-7b-instruct-v0.2-mlx** | 7B | ~6GB | ~40-50 | Good | âœ“ |
| **phi-3-medium-mlx** | 14B | ~10GB | ~20-25 | Very Good | âš ï¸ Tight |
| **qwen2.5-coder-14b-instruct-mlx** | 14B | ~10GB | ~25-30 | Excellent | âš ï¸ Tight |

### **2.2 Recommended Model: qwen2.5-coder-7b-instruct-mlx**

**Why This Model:**

âœ… **Optimized for 16GB RAM**
- Base model: ~6GB
- Leaves ~10GB for other processes
- Comfortable headroom

âœ… **MLX Optimization**
- 40-50 tokens/sec on M4
- Efficient memory usage
- GPU acceleration

âœ… **Code Generation Excellence**
- Trained specifically for code
- Understands Python, YAML, Markdown
- Good at following specifications

âœ… **Context Window**
- 32K tokens (sufficient for agent specs)
- Can handle large specifications

âœ… **Instruction Following**
- Excellent at tool calling
- Follows structured output formats
- Good at reasoning

### **2.3 Alternative: deepseek-coder-6.7b-instruct-mlx**

**Consider if:**
- Need faster inference (~45-55 tokens/sec)
- Want slightly lower memory footprint (~5.5GB)
- Prioritize speed over marginal quality difference

**Trade-offs:**
- Slightly smaller context window (16K vs 32K)
- Less well-known, fewer community resources

### **2.4 NOT Recommended (for 16GB)**

âŒ **qwen2.5-coder-14b-instruct-mlx**
- Too large (~10GB)
- Only ~6GB left for everything else
- Risk of memory pressure and swapping
- Slower inference (~25-30 tokens/sec)

âŒ **qwen2.5-coder-32b-instruct**
- Impossible to run on 16GB
- Requires 20GB+ RAM

### **2.5 LM Studio Configuration**

```
Recommended Settings for qwen2.5-coder-7b-instruct-mlx:

Model Parameters:
â”œâ”€ Context Length: 8192 (sufficient, saves memory vs 32K)
â”œâ”€ Temperature: 0.1 (deterministic for code generation)
â”œâ”€ Top P: 0.95
â”œâ”€ Top K: 40
â”œâ”€ Max Tokens: 4096 (per response)
â”œâ”€ Stop Sequences: ["```\n\n", "###"]
â””â”€ Repeat Penalty: 1.1

Server Settings:
â”œâ”€ Port: 1234 (default)
â”œâ”€ Enable CORS: Yes
â”œâ”€ API Type: OpenAI Compatible
â”œâ”€ GPU Layers: Auto (use all available)
â””â”€ Thread Count: 8 (for M4 10-core)

Memory Settings:
â”œâ”€ Model Memory: ~6GB
â”œâ”€ GPU Memory: Use unified memory
â””â”€ Batch Size: 512 (default)
```

---

## 3. Architecture Overview

### **3.1 System Components**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MacBook Pro M4 (16GB)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LM Studio (Port 1234)                                 â”‚ â”‚
â”‚  â”‚  Model: qwen2.5-coder-7b-instruct-mlx (~6GB)          â”‚ â”‚
â”‚  â”‚  API: OpenAI Compatible                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  META-AGENT (~2GB)                                     â”‚ â”‚
â”‚  â”‚  â”œâ”€ Python 3.11                                        â”‚ â”‚
â”‚  â”‚  â”œâ”€ LangChain + LangGraph                             â”‚ â”‚
â”‚  â”‚  â”œâ”€ 15 Tool Functions                                  â”‚ â”‚
â”‚  â”‚  â””â”€ Agent Writer Logic                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                 â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PostgreSQL      â”‚  â”‚  Docker Desktop (~2GB)         â”‚  â”‚
â”‚  â”‚  (~1GB)          â”‚  â”‚  â”œâ”€ Sandbox Containers         â”‚  â”‚
â”‚  â”‚  - Properties    â”‚  â”‚  â””â”€ Python 3.11-slim           â”‚  â”‚
â”‚  â”‚  - Metrics       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  - Formula Lib   â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                              â”‚
â”‚  OUTPUT:                                                     â”‚
â”‚  â”œâ”€ Generated Agents (src/agents/)                         â”‚
â”‚  â”œâ”€ Tests (tests/)                                          â”‚
â”‚  â”œâ”€ Specs (agent_specs/)                                   â”‚
â”‚  â””â”€ Docs (docs/)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3.2 Data Flow**

```
USER INPUT (Natural Language)
    â†“
Meta-Agent (Python)
    â†“
    â”œâ”€â†’ LM Studio API (http://localhost:1234/v1)
    â”‚   â”œâ”€ Requirements Analysis
    â”‚   â”œâ”€ Architecture Design
    â”‚   â”œâ”€ Specification Generation
    â”‚   â”œâ”€ Code Generation
    â”‚   â””â”€ Test Generation
    â”‚
    â”œâ”€â†’ PostgreSQL (localhost:5432)
    â”‚   â””â”€ Read existing schema (properties, financial_metrics)
    â”‚
    â””â”€â†’ Docker (localhost)
        â””â”€ Sandbox execution (for custom mode testing)
    â†“
GENERATED AGENTS
    â†“
    â”œâ”€â†’ DataAgent
    â”‚   â””â”€â†’ PostgreSQL (fetch data)
    â”‚
    â””â”€â†’ CalcAgent
        â”œâ”€â†’ DataAgent (tool call)
        â”œâ”€â†’ LM Studio (analysis, decisions)
        â””â”€â†’ Docker (sandbox for custom code)
```

---

## 4. Implementation Phases

### **Phase 0: Environment Setup (Day 1)**

**Duration**: 2-3 hours

**Tasks**:
1. Install/Update Software
2. Configure LM Studio
3. Download Model
4. Setup Project Structure
5. Test Connections

**Deliverables**:
- âœ“ All software installed
- âœ“ LM Studio running with model
- âœ“ PostgreSQL accessible
- âœ“ Docker running
- âœ“ Python environment ready

### **Phase 1: Meta-Agent Foundation (Days 2-3)**

**Duration**: 2 days

**Tasks**:
1. Build Tool System (15 tools)
2. Implement Meta-Agent Core
3. Create Prompt Templates
4. Setup LLM Integration
5. Build Validation Framework

**Deliverables**:
- âœ“ 15 tool functions implemented
- âœ“ Meta-Agent base class
- âœ“ LLM client wrapper
- âœ“ Validation utilities

### **Phase 2: Simple Agent Generation (Days 4-5)**

**Duration**: 2 days

**Tasks**:
1. Implement Requirements Analyzer
2. Implement Architecture Designer
3. Build Spec Generator
4. Build Code Generator
5. Test with Simple DSCR Request

**Deliverables**:
- âœ“ Can generate DataAgent
- âœ“ Can generate simple CalcAgent
- âœ“ Tests pass for generated agents
- âœ“ Documentation generated

### **Phase 3: Complex Agent Generation (Days 6-8)**

**Duration**: 3 days

**Tasks**:
1. Enhance Code Generator for Multi-Mode
2. Implement Code Validator
3. Build Sandbox Executor Integration
4. Implement Error Recovery
5. Test with Complex Multi-Mode Request

**Deliverables**:
- âœ“ Can generate multi-mode CalcAgent
- âœ“ Code validation working
- âœ“ Sandbox execution working
- âœ“ Error recovery functional

### **Phase 4: Integration & Testing (Days 9-10)**

**Duration**: 2 days

**Tasks**:
1. End-to-end Testing
2. Performance Optimization
3. Documentation
4. User Interface (CLI)
5. Demo Preparation

**Deliverables**:
- âœ“ Full system tested
- âœ“ Performance acceptable
- âœ“ Complete documentation
- âœ“ CLI interface
- âœ“ Demo ready

---

## 5. Detailed Setup Instructions

### **5.1 Initial Setup (Phase 0)**

#### **Step 1: Install Core Software**

```bash
# 1. Install Homebrew (if not installed)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# 2. Install Python 3.11
brew install python@3.11

# 3. Verify Python version
python3.11 --version  # Should be 3.11.x

# 4. Install PostgreSQL (if not already installed)
brew install postgresql@15

# 5. Start PostgreSQL
brew services start postgresql@15

# 6. Install Docker Desktop for Mac (Apple Silicon)
# Download from: https://www.docker.com/products/docker-desktop/
# Install and start Docker Desktop

# 7. Verify Docker
docker --version
docker ps  # Should show no errors
```

#### **Step 2: Install LM Studio**

```bash
# 1. Download LM Studio
# Visit: https://lmstudio.ai/
# Download macOS version (Apple Silicon)

# 2. Install application
# Drag to Applications folder

# 3. Launch LM Studio
# Open LM Studio from Applications

# 4. Download Model
In LM Studio:
  â†’ Search: "qwen2.5-coder-7b-instruct-mlx"
  â†’ Download the MLX version (optimized for Apple Silicon)
  â†’ Wait for download (takes 5-10 minutes)

# 5. Load Model
  â†’ Click on model in "My Models"
  â†’ Click "Load Model"
  â†’ Wait for model to load (~30 seconds)

# 6. Start Server
  â†’ Click "Local Server" tab
  â†’ Configure:
     - Port: 1234
     - Context Length: 8192
     - Temperature: 0.1
     - Max Tokens: 4096
  â†’ Click "Start Server"
  â†’ Server should show "Running on http://localhost:1234"

# 7. Test Server
curl http://localhost:1234/v1/models

# Expected output:
# {"object":"list","data":[{"id":"qwen2.5-coder-7b-instruct-mlx",...}]}
```

#### **Step 3: Setup Project Structure**

```bash
# 1. Navigate to project directory
cd "/Users/mohan_cr/Desktop/WinPra/Codebase/AgenticPOC_New"

# 2. Create project structure
mkdir -p meta_agent/tools
mkdir -p meta_agent/prompts
mkdir -p meta_agent/validators
mkdir -p meta_agent/utils
mkdir -p agent_specs
mkdir -p src/agents
mkdir -p tests/agents
mkdir -p tests/meta_agent
mkdir -p docs/agents
mkdir -p logs
mkdir -p sandbox

# 3. Create Python virtual environment
python3.11 -m venv venv

# 4. Activate virtual environment
source venv/bin/activate

# 5. Upgrade pip
pip install --upgrade pip

# 6. Create requirements.txt
cat > requirements.txt << 'EOF'
# LLM & Agent Framework
langchain==0.1.0
langchain-openai==0.0.2
langgraph==0.0.20
langchain-core==0.1.10

# Data Validation
pydantic==2.5.0
pydantic-settings==2.1.0

# Database
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
asyncpg==0.29.0

# Utilities
loguru==0.7.2
python-dotenv==1.0.0
pyyaml==6.0.1
httpx==0.25.2
requests==2.31.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0

# Code Analysis
ast-comments==1.1.2
black==23.12.0
isort==5.13.2

# Docker SDK (for sandbox)
docker==7.0.0

# Progress & CLI
tqdm==4.66.1
click==8.1.7
rich==13.7.0
EOF

# 7. Install dependencies
pip install -r requirements.txt

# 8. Create .env file
cat > .env << 'EOF'
# LM Studio Configuration
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL_NAME=qwen2.5-coder-7b-instruct-mlx
LLM_API_KEY=lm-studio
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4096

# Database Configuration (use existing)
DATABASE_URL=postgresql://username:password@localhost:5432/orlando_db

# Meta-Agent Configuration
META_AGENT_LOG_LEVEL=INFO
META_AGENT_MAX_RETRIES=3
META_AGENT_TIMEOUT=300

# Sandbox Configuration
DOCKER_TIMEOUT=30
SANDBOX_MEMORY_LIMIT=512m
SANDBOX_CPU_LIMIT=1.0
EOF

# 9. Update DATABASE_URL with your actual credentials
# Edit .env and replace username, password, and database name
```

#### **Step 4: Verify Existing PostgreSQL Data**

```bash
# 1. Connect to PostgreSQL
psql -U your_username -d orlando_db

# 2. Verify tables exist
\dt

# Expected output should include:
#   properties
#   financial_metrics

# 3. Check sample data
SELECT id, property_name, location FROM properties LIMIT 5;

SELECT property_id, annual_noi, annual_debt_service 
FROM financial_metrics LIMIT 5;

# 4. Exit psql
\q
```

#### **Step 5: Test All Connections**

```bash
# Create test script
cat > test_connections.py << 'EOF'
import os
from dotenv import load_dotenv
import httpx
from sqlalchemy import create_engine, text

load_dotenv()

# Test 1: LM Studio
print("Testing LM Studio...")
try:
    response = httpx.get(f"{os.getenv('LLM_BASE_URL')}/models", timeout=5)
    if response.status_code == 200:
        print("âœ“ LM Studio: Connected")
    else:
        print(f"âœ— LM Studio: Error {response.status_code}")
except Exception as e:
    print(f"âœ— LM Studio: {e}")

# Test 2: PostgreSQL
print("\nTesting PostgreSQL...")
try:
    engine = create_engine(os.getenv('DATABASE_URL'))
    with engine.connect() as conn:
        result = conn.execute(text("SELECT COUNT(*) FROM properties"))
        count = result.scalar()
        print(f"âœ“ PostgreSQL: Connected ({count} properties)")
except Exception as e:
    print(f"âœ— PostgreSQL: {e}")

# Test 3: Docker
print("\nTesting Docker...")
try:
    import docker
    client = docker.from_env()
    client.ping()
    print("âœ“ Docker: Connected")
except Exception as e:
    print(f"âœ— Docker: {e}")

print("\n" + "="*50)
print("Setup verification complete!")
EOF

# Run test
python test_connections.py
```

**Expected Output**:
```
Testing LM Studio...
âœ“ LM Studio: Connected

Testing PostgreSQL...
âœ“ PostgreSQL: Connected (10 properties)

Testing Docker...
âœ“ Docker: Connected

==================================================
Setup verification complete!
```

---

## 6. Resource Management

### **6.1 Memory Optimization**

#### **Strategy 1: Sequential Processing**

```
Avoid Parallel Operations:
  âœ— BAD: Generate multiple agents simultaneously
  âœ“ GOOD: Generate one agent at a time
  
  Reason: Each LLM call uses memory for:
    - Model weights (static, ~6GB)
    - Context window (dynamic, varies)
    - Response buffer
  
  Sequential = Lower peak memory usage
```

#### **Strategy 2: Context Window Management**

```
Limit Context Size:
  - Use 8192 tokens instead of 32K
  - Saves ~1-2GB of memory
  - Still sufficient for agent specs
  
  Break Large Specs:
  - If spec > 6000 tokens:
    â†’ Generate in components
    â†’ Combine after generation
```

#### **Strategy 3: Model Unloading**

```
IF memory pressure detected:
  1. Complete current generation
  2. Unload model from LM Studio
  3. Free up ~6GB
  4. Continue with validation/testing
  5. Reload model when needed

Detection:
  - Monitor with: vm_stat | grep "Pages free"
  - Alert if free memory < 2GB
```

#### **Strategy 4: Docker Memory Limits**

```
Sandbox Container Limits:
  - Memory: 512MB (sufficient for code execution)
  - CPU: 1.0 core
  - Timeout: 30 seconds
  
Docker Desktop Settings:
  - Memory: 4GB max
  - CPUs: 4 cores max
  - Swap: 2GB
```

### **6.2 Performance Optimization**

#### **LLM Inference Speed**

```
Expected Performance (qwen2.5-coder-7b-instruct-mlx on M4):
â”œâ”€ Tokens/sec: 40-50
â”œâ”€ Average response (500 tokens): ~10-12 seconds
â”œâ”€ Large code gen (2000 tokens): ~40-50 seconds
â””â”€ Spec generation (1000 tokens): ~20-25 seconds

Total Generation Time Estimates:
â”œâ”€ Simple Agent (DataAgent):
â”‚   - Spec generation: ~25 seconds
â”‚   - Code generation: ~45 seconds
â”‚   - Test generation: ~30 seconds
â”‚   - Total: ~1.5-2 minutes
â”‚
â””â”€ Complex Agent (Multi-mode CalcAgent):
    - Spec generation: ~35 seconds
    - Code generation (8 files): ~6-8 minutes
    - Test generation: ~2 minutes
    - Total: ~8-10 minutes

Bottlenecks:
  1. LLM inference (largest)
  2. Code validation (minimal)
  3. Test execution (varies)
  4. File I/O (negligible)
```

#### **Caching Strategy**

```
Cache Generated Components:
â”œâ”€ Template code (reduce regeneration)
â”œâ”€ Common patterns (imports, base classes)
â”œâ”€ Validation results (skip if code unchanged)
â””â”€ Test fixtures (reuse across tests)

Implementation:
  - Use file-based cache (disk)
  - Hash specs as cache key
  - Invalidate on spec change
  
Expected Speed Improvement:
  - 20-30% faster for similar requests
  - 50%+ faster for exact duplicates
```

### **6.3 Monitoring Commands**

```bash
# Monitor Memory Usage
watch -n 1 "ps aux | grep -E '(LM Studio|Python|postgres|docker)' | grep -v grep"

# Monitor GPU/Neural Engine
sudo powermetrics --samplers gpu_power -n 1

# Monitor Docker
docker stats

# Monitor PostgreSQL
psql -c "SELECT * FROM pg_stat_activity"

# Check LM Studio Status
curl http://localhost:1234/v1/models

# Monitor Python Process
top -pid $(pgrep -f "meta_agent")
```

---

## 7. Testing Strategy

### **7.1 Unit Tests**

```
Test Coverage:
â”œâ”€ Tool Functions (15 tools)
â”‚   â”œâ”€ analyze_requirements
â”‚   â”œâ”€ design_agent_architecture
â”‚   â”œâ”€ generate_agent_specification
â”‚   â””â”€ ... (12 more)
â”‚
â”œâ”€ Meta-Agent Core
â”‚   â”œâ”€ Request parsing
â”‚   â”œâ”€ Tool selection
â”‚   â”œâ”€ Error handling
â”‚   â””â”€ Result aggregation
â”‚
â””â”€ Validators
    â”œâ”€ Spec validator
    â”œâ”€ Code syntax validator
    â””â”€ Security validator

Target Coverage: 80%+

Test Execution:
  pytest tests/meta_agent/ -v --cov=meta_agent
```

### **7.2 Integration Tests**

```
Test Scenarios:
â”œâ”€ Scenario 1: Simple DSCR Agent
â”‚   Input: "Calculate DSCR from PostgreSQL"
â”‚   Expected: DataAgent + CalcAgent generated
â”‚   Time: ~4 minutes
â”‚
â”œâ”€ Scenario 2: Multi-Mode DSCR Agent
â”‚   Input: "DSCR with 3 modes and LLM analysis"
â”‚   Expected: DataAgent + Complex CalcAgent
â”‚   Time: ~12 minutes
â”‚
â””â”€ Scenario 3: Custom Requirements
    Input: "DSCR with 15% stress test"
    Expected: Agents with custom code generation
    Time: ~15 minutes

Test Execution:
  pytest tests/integration/ -v -s --timeout=900
```

### **7.3 Generated Agent Tests**

```
After Meta-Agent generates agents:
â”œâ”€ Auto-generated unit tests run automatically
â”œâ”€ Verify all tests pass
â””â”€ Check code coverage

Example:
  Generated: src/agents/data_agent.py
  Auto-generated: tests/agents/test_data_agent.py
  Run: pytest tests/agents/test_data_agent.py
  Expected: All tests pass
```

### **7.4 Performance Tests**

```
Measure:
â”œâ”€ Generation time per agent type
â”œâ”€ Memory usage during generation
â”œâ”€ LLM token usage
â””â”€ Docker sandbox overhead

Benchmarks:
â”œâ”€ Simple agent: < 3 minutes
â”œâ”€ Complex agent: < 15 minutes
â”œâ”€ Memory peak: < 14GB (leaving 2GB buffer)
â””â”€ LLM calls: < 20 per complex agent

Test Execution:
  python tests/performance/benchmark_generation.py
```

---

## 8. Timeline & Milestones

### **Detailed Implementation Schedule**

```
Week 1: Foundation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Day 1 (Monday): Environment Setup
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Install all software (2 hours)
  â˜ Configure LM Studio (1 hour)
  â˜ Setup project structure (30 min)
  â˜ Test all connections (30 min)

Milestone: âœ“ Environment ready, all tests pass

Day 2 (Tuesday): Tool System - Part 1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Implement tool base class (1 hour)
  â˜ Implement 5 tools:
     - analyze_requirements (1.5 hours)
     - design_agent_architecture (1.5 hours)
     - generate_agent_specification (1.5 hours)
     - validate_specification (1 hour)
     - write_file (30 min)
  â˜ Write unit tests (1.5 hours)

Milestone: âœ“ 5 tools working and tested

Day 3 (Wednesday): Tool System - Part 2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Implement 10 more tools:
     - generate_agent_code (2 hours)
     - validate_code_syntax (1 hour)
     - validate_code_security (1.5 hours)
     - generate_unit_tests (1.5 hours)
     - run_tests (1 hour)
     - read_file (30 min)
     - create_directory (30 min)
     - generate_documentation (1 hour)
     - deploy_agent (1 hour)
     - verify_agent_health (30 min)
  â˜ Integration testing (1 hour)

Milestone: âœ“ All 15 tools implemented and tested

Day 4 (Thursday): Meta-Agent Core
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Build Meta-Agent base class (2 hours)
  â˜ Implement LLM client wrapper (1.5 hours)
  â˜ Build prompt template system (1.5 hours)
  â˜ Implement tool orchestration (2 hours)
  â˜ Add logging and monitoring (1 hour)

Milestone: âœ“ Meta-Agent can make tool calls

Day 5 (Friday): Simple Agent Generation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Build requirements analyzer (1.5 hours)
  â˜ Build architecture designer (1.5 hours)
  â˜ Implement spec generator (2 hours)
  â˜ Implement code generator (2 hours)
  â˜ Test with simple DSCR request (1 hour)

Milestone: âœ“ Can generate simple DataAgent

Week 1 Summary:
  Time: 40 hours
  Deliverables:
    âœ“ 15 tools implemented
    âœ“ Meta-Agent core functional
    âœ“ Can generate simple agents


Week 2: Advanced Features & Testing
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Day 6 (Monday): Complex Agent Support
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Enhance code generator for multi-component agents (3 hours)
  â˜ Implement modular code generation (2 hours)
  â˜ Add component dependency resolution (2 hours)
  â˜ Test with multi-component CalcAgent (1 hour)

Milestone: âœ“ Can generate complex multi-file agents

Day 7 (Tuesday): Validation & Security
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Build comprehensive code validator (2 hours)
  â˜ Implement security checker (2 hours)
  â˜ Add syntax validator (1 hour)
  â˜ Build test for validation pipeline (1.5 hours)
  â˜ Test with intentionally bad code (1.5 hours)

Milestone: âœ“ Validation catches all security issues

Day 8 (Wednesday): Sandbox Execution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Build Docker sandbox wrapper (2 hours)
  â˜ Implement code injection (1.5 hours)
  â˜ Add resource limiting (1 hour)
  â˜ Implement result extraction (1 hour)
  â˜ Test sandbox with custom code (2 hours)
  â˜ Error handling for sandbox failures (1.5 hours)

Milestone: âœ“ Custom code executes safely in sandbox

Day 9 (Thursday): Error Recovery & Optimization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ Implement retry logic (1.5 hours)
  â˜ Build error analysis (2 hours)
  â˜ Add self-correction (2 hours)
  â˜ Optimize LLM prompts (1.5 hours)
  â˜ Add caching layer (1.5 hours)
  â˜ Performance testing (1.5 hours)

Milestone: âœ“ System recovers from errors automatically

Day 10 (Friday): Integration & Polish
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tasks:
  â˜ End-to-end testing (2 hours)
  â˜ Build CLI interface (2 hours)
  â˜ Write comprehensive documentation (2 hours)
  â˜ Performance benchmarking (1.5 hours)
  â˜ Create demo examples (1.5 hours)
  â˜ Final testing and bug fixes (1 hour)

Milestone: âœ“ Complete system ready for use

Week 2 Summary:
  Time: 40 hours
  Deliverables:
    âœ“ Full Meta-Agent system operational
    âœ“ Can generate any agent type
    âœ“ Error recovery functional
    âœ“ CLI interface ready
    âœ“ Complete documentation


Total Implementation Time: 80 hours (2 weeks)
```

### **Critical Path**

```
Dependencies:
  Day 1 â†’ Must complete before Day 2
  Days 2-3 â†’ Must complete before Day 4
  Day 4 â†’ Must complete before Day 5
  Days 5-6 â†’ Must complete before Day 8
  Day 8 â†’ Must complete before Day 9

Parallel Opportunities:
  - Documentation can be written alongside implementation
  - Unit tests can be written alongside tools
  - Some tools are independent and can be built in any order
```

### **Success Criteria**

```
Phase 0 Success:
  â˜ All software installed
  â˜ LM Studio running with model loaded
  â˜ PostgreSQL accessible with sample data
  â˜ Docker operational
  â˜ Test script passes all checks

Phase 1 Success:
  â˜ All 15 tools implemented
  â˜ Meta-Agent core functional
  â˜ Can make LLM calls successfully
  â˜ Logging system working

Phase 2 Success:
  â˜ Can generate simple DataAgent from NL request
  â˜ Generated agent code is valid
  â˜ Generated tests pass
  â˜ Generation time < 3 minutes

Phase 3 Success:
  â˜ Can generate complex multi-mode CalcAgent
  â˜ All components generated correctly
  â˜ Security validation works
  â˜ Sandbox execution works
  â˜ Generation time < 15 minutes

Phase 4 Success:
  â˜ End-to-end tests pass
  â˜ Error recovery works
  â˜ Performance meets benchmarks
  â˜ Documentation complete
  â˜ Demo ready
```

---

## 9. Usage Examples

### **9.1 Running Meta-Agent (After Implementation)**

```bash
# Activate environment
cd "/Users/mohan_cr/Desktop/WinPra/Codebase/AgenticPOC_New"
source venv/bin/activate

# Ensure LM Studio is running (check in app)

# Run Meta-Agent CLI
python meta_agent_cli.py

# Interactive mode
>>> Enter your request: I need an agent to calculate DSCR for properties

Meta-Agent: Analyzing request...
Meta-Agent: Requirements extracted
Meta-Agent: Designing 2-agent architecture
Meta-Agent: Generating DataAgent...
Meta-Agent: âœ“ DataAgent complete (1.8 minutes)
Meta-Agent: Generating CalcAgent...
Meta-Agent: âœ“ CalcAgent complete (2.1 minutes)
Meta-Agent: Running tests...
Meta-Agent: âœ“ All tests passed (4/4)
Meta-Agent: âœ“ System ready!

Generated files:
  - src/agents/data_agent.py
  - src/agents/calc_agent.py
  - tests/agents/test_data_agent.py
  - tests/agents/test_calc_agent.py
  - docs/agents/data_agent.md
  - docs/agents/calc_agent.md

Total time: 3 minutes 54 seconds

# Test generated agents
python -c "
from src.agents.calc_agent import CalcAgent
agent = CalcAgent()
result = agent.calculate_dscr(property_id=5)
print(f'DSCR: {result.dscr}, Status: {result.validation_status}')
"

Output:
  DSCR: 1.35, Status: PASS
```

### **9.2 Complex Request Example**

```bash
python meta_agent_cli.py

>>> Enter your request: Build a DSCR system with 3 modes - standard, 
    conservative (10% NOI haircut), and custom (user describes stress 
    tests). Include LLM analysis and adjustment suggestions.

Meta-Agent: Analyzing complex request...
Meta-Agent: Detected: HIGH complexity
Meta-Agent: Designing 2-agent architecture (11 components)
Meta-Agent: Generating DataAgent...
Meta-Agent: âœ“ DataAgent complete (1.9 minutes)
Meta-Agent: Generating CalcAgent (complex)...
  â†’ Generating main class... (45 seconds)
  â†’ Generating mode detector... (30 seconds)
  â†’ Generating standard calculator... (40 seconds)
  â†’ Generating library calculator... (50 seconds)
  â†’ Generating custom calculator... (2.5 minutes)
  â†’ Generating analysis engine... (1.2 minutes)
  â†’ Generating decision engine... (1.1 minutes)
  â†’ Generating report generator... (35 seconds)
Meta-Agent: âœ“ CalcAgent complete (8.2 minutes)
Meta-Agent: Generating tests...
Meta-Agent: âœ“ Tests generated (2.3 minutes)
Meta-Agent: Running tests...
  â†’ 23/25 tests passed
  â†’ 2 tests failed (analyzing...)
Meta-Agent: Fixing issues...
  â†’ Regenerating custom calculator (timeout fix)
  â†’ Regenerating decision engine (iteration fix)
Meta-Agent: Re-running tests...
Meta-Agent: âœ“ All tests passed (25/25)
Meta-Agent: âœ“ System ready!

Total time: 12 minutes 18 seconds
Total LLM calls: 18
Total tokens: ~45,000
```

---

## 10. Troubleshooting

### **10.1 Common Issues**

```
ISSUE: LM Studio not responding
SOLUTION:
  1. Check if server is running (green indicator)
  2. Restart LM Studio
  3. Reload model
  4. Test with: curl http://localhost:1234/v1/models

ISSUE: Out of memory errors
SOLUTION:
  1. Close unnecessary applications
  2. Reduce context length to 4096
  3. Process one agent at a time
  4. Monitor with: vm_stat

ISSUE: PostgreSQL connection refused
SOLUTION:
  1. Check if PostgreSQL is running:
     brew services list
  2. Start if needed:
     brew services start postgresql@15
  3. Verify connection:
     psql -U username -d orlando_db

ISSUE: Docker sandbox timeout
SOLUTION:
  1. Increase timeout in .env (DOCKER_TIMEOUT=60)
  2. Check Docker resource limits
  3. Restart Docker Desktop

ISSUE: Generated code has syntax errors
SOLUTION:
  1. Check LLM temperature (should be 0.1)
  2. Verify model is loaded correctly
  3. Meta-Agent should auto-fix (retry mechanism)
  4. If persists, regenerate with explicit instructions

ISSUE: Slow generation (>20 minutes)
SOLUTION:
  1. Check LM Studio GPU usage
  2. Reduce context length
  3. Close background apps
  4. Check if swapping (Activity Monitor)
```

---

## âœ… Final Recommendations

### **For 16GB M4 MacBook Pro**

**âœ… DO:**
- Use qwen2.5-coder-7b-instruct-mlx (best balance)
- Keep context length at 8192 (saves memory)
- Generate agents sequentially (not parallel)
- Monitor memory usage regularly
- Close unused applications during generation
- Use Docker resource limits

**âŒ DON'T:**
- Try to run 14B+ models (insufficient RAM)
- Generate multiple agents simultaneously
- Use 32K context unless necessary
- Run without monitoring memory
- Skip validation steps (causes rework)

### **Expected Performance**

```
Simple Agent Generation:
  Time: 2-4 minutes
  Memory: Peak 12GB
  Success Rate: >95%

Complex Agent Generation:
  Time: 10-15 minutes
  Memory: Peak 14GB
  Success Rate: >90% (with retry)

Resource Usage:
  CPU: 60-80% during LLM inference
  GPU: 80-95% during LLM inference
  Memory: 12-14GB peak
  Disk I/O: Minimal
```

### **Next Steps After Implementation**

```
1. Run integration tests with existing PostgreSQL data
2. Generate 2-3 different agent systems to validate
3. Benchmark performance and optimize
4. Create library of pre-generated agent specs
5. Build web UI (optional, future enhancement)
6. Integrate with CI/CD (optional)
```

---

**Ready to build! Start with Phase 0 (Day 1) and follow the timeline.** ðŸš€

**Estimated total implementation time: 80 hours (2 weeks at 8 hours/day)**

